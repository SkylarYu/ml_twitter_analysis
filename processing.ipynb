{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/rl3279/twitter_sentiment_analysis for organized code. \n",
    "\n",
    "Main code used in this notebook is distributed into two files:\n",
    "- [`preprocessing.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/preprocessing.py), containing all scripts to clean and to process raw tweeter text string. \n",
    "- [`feature_engineering.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/feature_engineering.py), containing scripts to extract features from the processed text.\n",
    "\n",
    "For reference, the exact content of these files will be attached at the very end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_globals\n",
    "from preprocessing import cleaning, preprocess_pipeline\n",
    "import feature_engineering as fe\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will only showcase the cleaning, preprocessing, feature-engineering workflow on a subset of data, since the resulting dataset is enormous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linru\\AppData\\Roaming\\Python\\Python39\\site-packages\\dateutil\\parser\\_parser.py:1213: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>weekday_Mon</th>\n",
       "      <th>weekday_Tue</th>\n",
       "      <th>weekday_Wed</th>\n",
       "      <th>weekday_Thu</th>\n",
       "      <th>weekday_Fri</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_ðº</th>\n",
       "      <th>tok_ðºðµ</th>\n",
       "      <th>tok_ð¼ð</th>\n",
       "      <th>tok_ð¼ðµð½ñ</th>\n",
       "      <th>tok_ð¼ð½ðµ</th>\n",
       "      <th>tok_ð½ð</th>\n",
       "      <th>tok_ð½ð³ñ</th>\n",
       "      <th>tok_ð½ñ</th>\n",
       "      <th>tok_ð¾ð</th>\n",
       "      <th>tok_ð¾ñ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2175288554</td>\n",
       "      <td>Mon Jun 15 00:34:44 PDT 2009</td>\n",
       "      <td>Fatimaa14</td>\n",
       "      <td>Goodnight tweeets  its too late but i waas lis...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1771159237</td>\n",
       "      <td>Mon May 11 23:23:24 PDT 2009</td>\n",
       "      <td>MissKRYP2NT</td>\n",
       "      <td>ps. @pumpkyn once again u r kickin my ass w th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1574060114</td>\n",
       "      <td>Tue Apr 21 04:05:03 PDT 2009</td>\n",
       "      <td>JBear2978</td>\n",
       "      <td>My heart is in Maine</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2064836548</td>\n",
       "      <td>Sun Jun 07 07:17:20 PDT 2009</td>\n",
       "      <td>raine_angel</td>\n",
       "      <td>5 days on antibiotics and my pain is still a 10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2065993936</td>\n",
       "      <td>Sun Jun 07 09:41:14 PDT 2009</td>\n",
       "      <td>heathermoire</td>\n",
       "      <td>One year ago today, I walked into the MSPCA an...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4</td>\n",
       "      <td>1793309239</td>\n",
       "      <td>Thu May 14 02:11:27 PDT 2009</td>\n",
       "      <td>twistingaether</td>\n",
       "      <td>@marcthom Good! Do you like my scarf?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>1993677299</td>\n",
       "      <td>Mon Jun 01 10:45:32 PDT 2009</td>\n",
       "      <td>maaangelaaa</td>\n",
       "      <td>Busy, busy, busy at work. The weekend is only ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0</td>\n",
       "      <td>1882235764</td>\n",
       "      <td>Fri May 22 06:29:52 PDT 2009</td>\n",
       "      <td>TeamGiles</td>\n",
       "      <td>http://yfrog.com/5au8dj oh no! Knocked over my...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4</td>\n",
       "      <td>2058500604</td>\n",
       "      <td>Sat Jun 06 15:03:58 PDT 2009</td>\n",
       "      <td>AmyJessicaB</td>\n",
       "      <td>@erica15brown ohh mann you've gotta love fall ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>2066344614</td>\n",
       "      <td>Sun Jun 07 10:20:32 PDT 2009</td>\n",
       "      <td>Kikirowr</td>\n",
       "      <td>@Cadistra the second kind  the worst~ let me s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 8278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target         ids                          date            user  \\\n",
       "0          4  2175288554  Mon Jun 15 00:34:44 PDT 2009       Fatimaa14   \n",
       "1          4  1771159237  Mon May 11 23:23:24 PDT 2009     MissKRYP2NT   \n",
       "2          0  1574060114  Tue Apr 21 04:05:03 PDT 2009       JBear2978   \n",
       "3          0  2064836548  Sun Jun 07 07:17:20 PDT 2009     raine_angel   \n",
       "4          4  2065993936  Sun Jun 07 09:41:14 PDT 2009    heathermoire   \n",
       "...      ...         ...                           ...             ...   \n",
       "4995       4  1793309239  Thu May 14 02:11:27 PDT 2009  twistingaether   \n",
       "4996       0  1993677299  Mon Jun 01 10:45:32 PDT 2009     maaangelaaa   \n",
       "4997       0  1882235764  Fri May 22 06:29:52 PDT 2009       TeamGiles   \n",
       "4998       4  2058500604  Sat Jun 06 15:03:58 PDT 2009     AmyJessicaB   \n",
       "4999       0  2066344614  Sun Jun 07 10:20:32 PDT 2009        Kikirowr   \n",
       "\n",
       "                                                   text  weekday_Mon  \\\n",
       "0     Goodnight tweeets  its too late but i waas lis...          1.0   \n",
       "1     ps. @pumpkyn once again u r kickin my ass w th...          1.0   \n",
       "2                                 My heart is in Maine           0.0   \n",
       "3      5 days on antibiotics and my pain is still a 10           0.0   \n",
       "4     One year ago today, I walked into the MSPCA an...          0.0   \n",
       "...                                                 ...          ...   \n",
       "4995             @marcthom Good! Do you like my scarf?           0.0   \n",
       "4996  Busy, busy, busy at work. The weekend is only ...          1.0   \n",
       "4997  http://yfrog.com/5au8dj oh no! Knocked over my...          0.0   \n",
       "4998  @erica15brown ohh mann you've gotta love fall ...          0.0   \n",
       "4999  @Cadistra the second kind  the worst~ let me s...          0.0   \n",
       "\n",
       "      weekday_Tue  weekday_Wed  weekday_Thu  weekday_Fri  ...  tok_ðº  \\\n",
       "0             0.0          0.0          0.0          0.0  ...     0.0   \n",
       "1             0.0          0.0          0.0          0.0  ...     0.0   \n",
       "2             1.0          0.0          0.0          0.0  ...     0.0   \n",
       "3             0.0          0.0          0.0          0.0  ...     0.0   \n",
       "4             0.0          0.0          0.0          0.0  ...     0.0   \n",
       "...           ...          ...          ...          ...  ...     ...   \n",
       "4995          0.0          0.0          1.0          0.0  ...     0.0   \n",
       "4996          0.0          0.0          0.0          0.0  ...     0.0   \n",
       "4997          0.0          0.0          0.0          1.0  ...     0.0   \n",
       "4998          0.0          0.0          0.0          0.0  ...     0.0   \n",
       "4999          0.0          0.0          0.0          0.0  ...     0.0   \n",
       "\n",
       "      tok_ðºðµ tok_ð¼ð tok_ð¼ðµð½ñ  tok_ð¼ð½ðµ  tok_ð½ð  tok_ð½ð³ñ  tok_ð½ñ  \\\n",
       "0          0.0     0.0         0.0         0.0      0.0        0.0      0.0   \n",
       "1          0.0     0.0         0.0         0.0      0.0        0.0      0.0   \n",
       "2          0.0     0.0         0.0         0.0      0.0        0.0      0.0   \n",
       "3          0.0     0.0         0.0         0.0      0.0        0.0      0.0   \n",
       "4          0.0     0.0         0.0         0.0      0.0        0.0      0.0   \n",
       "...        ...     ...         ...         ...      ...        ...      ...   \n",
       "4995       0.0     0.0         0.0         0.0      0.0        0.0      0.0   \n",
       "4996       0.0     0.0         0.0         0.0      0.0        0.0      0.0   \n",
       "4997       0.0     0.0         0.0         0.0      0.0        0.0      0.0   \n",
       "4998       0.0     0.0         0.0         0.0      0.0        0.0      0.0   \n",
       "4999       0.0     0.0         0.0         0.0      0.0        0.0      0.0   \n",
       "\n",
       "      tok_ð¾ð  tok_ð¾ñ  \n",
       "0         0.0      0.0  \n",
       "1         0.0      0.0  \n",
       "2         0.0      0.0  \n",
       "3         0.0      0.0  \n",
       "4         0.0      0.0  \n",
       "...       ...      ...  \n",
       "4995      0.0      0.0  \n",
       "4996      0.0      0.0  \n",
       "4997      0.0      0.0  \n",
       "4998      0.0      0.0  \n",
       "4999      0.0      0.0  \n",
       "\n",
       "[5000 rows x 8278 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_sub_dataset\n",
    "DATA_PATH = \"/\".join([my_globals.DATA_DIR, my_globals.MAIN_DATA_NAME])\n",
    "\n",
    "get_sub_dataset(size = 5000, random_seed=42)\n",
    "data = pd.read_csv(\n",
    "    \"/\".join([my_globals.DATA_DIR, \"twitter_seed42.csv\"]),\n",
    "    encoding = \"latin1\",\n",
    "    header = 0, \n",
    ")\n",
    "\n",
    "\n",
    "data = cleaning(data)\n",
    "data[\"processed_text\"] = data[\"text\"].apply(preprocess_pipeline)\n",
    "\n",
    "data[\"exclaim_freq\"] = data[\"text\"].apply(fe.exclaim_freq)\n",
    "data[\"mention_count\"] = data[\"text\"].apply(fe.mention_count)\n",
    "data[\"cap_freq\"] = data[\"text\"].apply(fe.cap_freq)\n",
    "tfidf = fe.get_tfidf(data[\"processed_text\"])\n",
    "data = pd.concat([data, tfidf], axis = 1)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing \n",
    "steps include:\n",
    "- Link deletion\n",
    "- Username mentions deletion\n",
    "- Decontraction (\"can't\" -> \"cannot\")\n",
    "- Lemmetization (\"shoes\" -> \"shoe\"; \"mice\" -> \"mouse\")\n",
    "- Stopwords deletion (*e.g.* \"a\", \"the\", \"to\", \"of\", etc.)\n",
    "- Punctuation deletion\n",
    "- Digits deletion\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`preprocessing.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/preprocessing.py) content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_globals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "from functools import reduce\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "def setup_nltk():\n",
    "    \"\"\"Downloads necessary packages within nltk\"\"\"\n",
    "    packages = [\"punkt\", \"wordnet\", \"stopwords\"]\n",
    "    for p in packages:\n",
    "        try:\n",
    "            nltk.data.find(p)\n",
    "        except LookupError:\n",
    "            nltk.download(p)\n",
    "\n",
    "\n",
    "def tokenize(s: str, how=\"word_tokenize\") -> List[str]:\n",
    "    if how == \"word_tokenize\":\n",
    "        return word_tokenize(s)\n",
    "    elif how == \"split\":\n",
    "        return s.split()\n",
    "\n",
    "\n",
    "def del_username(s: str) -> str:\n",
    "    \"\"\"Delete @Username from a tweet str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join([t for t in tokenize(s, how=\"split\") if not t.startswith(\"@\")])\n",
    "\n",
    "\n",
    "def del_punc(s: str) -> str:\n",
    "    \"\"\"Delete punctuations from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    punc = my_globals.PUNCS\n",
    "    return \"\".join([w for w in s if w not in punc])\n",
    "\n",
    "\n",
    "def del_link(s: str) -> str:\n",
    "    \"\"\"Delete links from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    r = r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)\"\n",
    "    return \" \".join([re.sub(r, \"\", t) for t in tokenize(s, how=\"split\")])\n",
    "\n",
    "\n",
    "def decontract(s: str) -> str:\n",
    "    \"\"\"Remove contractions in text.\n",
    "    e.g. I'm -> I am; she'd -> she would\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for t in tokenize(s, how=\"split\"):\n",
    "        tokens.append(contractions.fix(t))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def del_stopwords(s: str) -> str:\n",
    "    \"\"\"Delete stopwords and punctuation from a string.\n",
    "    Note that the type-hinting indicates that this function ought\n",
    "    to be run first in the pre-processing pipeline.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    return \" \".join([t for t in tokenize(s) if t not in stop_words])\n",
    "\n",
    "\n",
    "def del_digits(s: str) -> str:\n",
    "    \"\"\"Delete digits from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    return \" \".join([w for w in tokenize(s) if not w.isdigit()])\n",
    "\n",
    "\n",
    "def lemmatize(s: str) -> str:\n",
    "    \"\"\"Lemmatize str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(t) for t in tokenize(s)])\n",
    "\n",
    "\n",
    "def preprocess_pipeline(s: str, return_lower = True) -> str:\n",
    "    \"\"\"Run string through all pre-processing functions.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    s = reduce(\n",
    "        lambda value, function: function(value),\n",
    "        (\n",
    "            del_link,\n",
    "            del_username,\n",
    "            decontract,\n",
    "            lemmatize,\n",
    "            del_stopwords,\n",
    "            del_punc,\n",
    "            del_digits\n",
    "        ),\n",
    "        s,\n",
    "    )\n",
    "\n",
    "    return s.lower() if return_lower else s\n",
    "\n",
    "def str_datetime(s: str):\n",
    "    \"\"\"Parse and format a datetime str to weekday and datetime.MAXYEAR\n",
    "    \n",
    "    :param s: input string containing datetime information\n",
    "    :type s: str\n",
    "    :rtype: tuple[str, str]\n",
    "    \"\"\"\n",
    "    ss = parse(s).strftime('%a %Y-%m-%d %H:%M:%S')\n",
    "    return ss[:3], ss[4:]\n",
    "\n",
    "\n",
    "def cleaning(df: pd.DataFrame):\n",
    "    \"\"\"Cleaning script of the data (or subset).\n",
    "    \n",
    "    :param df: input dataframe.\n",
    "    :type df: pd.DataFrame\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Parse weekday and datetime\n",
    "    weekday_datetime = pd.DataFrame(\n",
    "        list(df.loc[:, \"date\"].apply(str_datetime)),\n",
    "        columns=[\"weekday\", \"datetime\"]\n",
    "    )\n",
    "    # One-hot encode weekday\n",
    "    weekdaydummies = pd.get_dummies(\n",
    "        weekday_datetime['weekday'], \n",
    "        prefix='weekday', \n",
    "        dtype=float\n",
    "    )\n",
    "    weekdaydummies = pd.DataFrame(\n",
    "        weekdaydummies, \n",
    "        columns=['weekday_'+w for w in [\n",
    "            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"\n",
    "        ]]\n",
    "    )\n",
    "    # Concatenate weekday dummies to other features\n",
    "    weekdaydummies_datetime = pd.concat(\n",
    "        [weekdaydummies, weekday_datetime['datetime']], \n",
    "        axis=1\n",
    "    )\n",
    "    df = pd.concat([df, weekdaydummies_datetime], axis=1)\n",
    "    # Drop the column with single unique value.\n",
    "    df.drop(\"flag\", axis = 1, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "proposed features include:\n",
    "- \"!\" usage frequency\n",
    "- \"@\" usage count \n",
    "- capitalized letter usage count\n",
    "- TF-IDF (Term Frequency - Inverse Document Frequency) from processed text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`feature_engineering.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/feature_engineering.py) content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing import preprocess_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "def exlaim_freq(s: str) -> float:\n",
    "    \"\"\"Frequency of excalamtion points in a tweet.\n",
    "\n",
    "    :param s: input str\n",
    "    :type s: str\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    s = \"\".join(s.split())\n",
    "    count = sum([1 if t == \"!\" else 0 for t in s])\n",
    "    return count / len(s)\n",
    "\n",
    "\n",
    "def mention_count(s: str) -> int:\n",
    "    \"\"\"Counts how many mentions occurred in a tweet.\n",
    "\n",
    "    :param s: input str\n",
    "    :type s: str\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    count = sum([1 if t.startswith(\"@\") else 0 for t in s.split()])\n",
    "    return count\n",
    "\n",
    "\n",
    "def cap_freq(s: str) -> float:\n",
    "    \"\"\"Frequency of capitalized letter usage in a tweet.\n",
    "\n",
    "    :param s: input str\n",
    "    :type s: str\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    s = preprocess_pipeline(s)\n",
    "    count = sum([1 if t.isupper() else 0 for t in s])\n",
    "    return count / len(s)\n",
    "\n",
    "\n",
    "def get_tfidf(data: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Encode a Series of text string to TF-IDF.\n",
    "\n",
    "    :param data: input data\n",
    "    :type data: pd.Series\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    tfidf = TfidfTransformer()\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    X = tfidf.fit_transform(X)\n",
    "    return X.toarray()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
