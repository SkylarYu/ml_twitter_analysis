{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/rl3279/twitter_sentiment_analysis for organized code. \n",
    "\n",
    "Main code used in this notebook is distributed into two files:\n",
    "- [`preprocessing.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/preprocessing.py), containing all scripts to clean and to process raw tweeter text string. \n",
    "- [`feature_engineering.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/feature_engineering.py), containing scripts to extract features from the processed text.\n",
    "\n",
    "For reference, the exact content of these files will be attached at the very end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_globals\n",
    "from preprocessing import cleaning, preprocess_pipeline\n",
    "import feature_engineering as fe\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will only showcase the cleaning, preprocessing, feature-engineering workflow on a subset of data, since the resulting dataset is enormous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linru\\AppData\\Roaming\\Python\\Python39\\site-packages\\dateutil\\parser\\_parser.py:1213: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>weekday_Mon</th>\n",
       "      <th>weekday_Tue</th>\n",
       "      <th>weekday_Wed</th>\n",
       "      <th>weekday_Thu</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_ð½ð¾ðµ</th>\n",
       "      <th>tfidf_ð½ñ</th>\n",
       "      <th>tfidf_ð¾</th>\n",
       "      <th>tfidf_ð¾ð</th>\n",
       "      <th>tfidf_ð¾ð²</th>\n",
       "      <th>tfidf_ð¾ðµ</th>\n",
       "      <th>tfidf_ð¾ðµð½</th>\n",
       "      <th>tfidf_ð¾ð½ð¾ð¹</th>\n",
       "      <th>tfidf_ð¾ð½ñ</th>\n",
       "      <th>tfidf_ð¾ñ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1031776</td>\n",
       "      <td>4</td>\n",
       "      <td>1933110114</td>\n",
       "      <td>Tue May 26 23:01:24 PDT 2009</td>\n",
       "      <td>WookieeChew</td>\n",
       "      <td>@jerryfee Interesting....I'll give it a shot!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>720217</td>\n",
       "      <td>0</td>\n",
       "      <td>2260825531</td>\n",
       "      <td>Sat Jun 20 19:49:58 PDT 2009</td>\n",
       "      <td>xokristinaxo</td>\n",
       "      <td>Just got back from the Puerto Rican Fest. Mmmm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1579676</td>\n",
       "      <td>4</td>\n",
       "      <td>2189971972</td>\n",
       "      <td>Tue Jun 16 01:24:27 PDT 2009</td>\n",
       "      <td>Curlybird1988</td>\n",
       "      <td>working today but 2 days off from tomro... lif...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183731</td>\n",
       "      <td>0</td>\n",
       "      <td>1967498264</td>\n",
       "      <td>Fri May 29 19:41:29 PDT 2009</td>\n",
       "      <td>JiMpiSh</td>\n",
       "      <td>@tommytrc @tvorse @LisaRedShoesPR The more I h...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1120719</td>\n",
       "      <td>4</td>\n",
       "      <td>1974001323</td>\n",
       "      <td>Sat May 30 12:14:48 PDT 2009</td>\n",
       "      <td>samsonlion</td>\n",
       "      <td>What a beautiful day. Too bad I missed half of...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>112171</td>\n",
       "      <td>0</td>\n",
       "      <td>1825363871</td>\n",
       "      <td>Sun May 17 05:25:27 PDT 2009</td>\n",
       "      <td>xoxolinzieoxox</td>\n",
       "      <td>i cnt believe how much tweets ive done bout th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>645683</td>\n",
       "      <td>0</td>\n",
       "      <td>2236341913</td>\n",
       "      <td>Fri Jun 19 03:27:52 PDT 2009</td>\n",
       "      <td>Cranialstrain</td>\n",
       "      <td>@o2 Guys, loving my iPhone but have to say the...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1011918</td>\n",
       "      <td>4</td>\n",
       "      <td>1881149672</td>\n",
       "      <td>Fri May 22 03:45:55 PDT 2009</td>\n",
       "      <td>reetsjel</td>\n",
       "      <td>&amp;quot;Do You Feel The Way You Hate? Do You Hat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1566265</td>\n",
       "      <td>4</td>\n",
       "      <td>2187731611</td>\n",
       "      <td>Mon Jun 15 20:43:13 PDT 2009</td>\n",
       "      <td>ZolaStapler</td>\n",
       "      <td>My blood test was excellent    I had my last c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1038542</td>\n",
       "      <td>4</td>\n",
       "      <td>1956687871</td>\n",
       "      <td>Thu May 28 22:26:41 PDT 2009</td>\n",
       "      <td>pcarles</td>\n",
       "      <td>canard65 has just commented in Fr my blog post...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 15926 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  target         ids                          date  \\\n",
       "0     1031776       4  1933110114  Tue May 26 23:01:24 PDT 2009   \n",
       "1      720217       0  2260825531  Sat Jun 20 19:49:58 PDT 2009   \n",
       "2     1579676       4  2189971972  Tue Jun 16 01:24:27 PDT 2009   \n",
       "3      183731       0  1967498264  Fri May 29 19:41:29 PDT 2009   \n",
       "4     1120719       4  1974001323  Sat May 30 12:14:48 PDT 2009   \n",
       "...       ...     ...         ...                           ...   \n",
       "4995   112171       0  1825363871  Sun May 17 05:25:27 PDT 2009   \n",
       "4996   645683       0  2236341913  Fri Jun 19 03:27:52 PDT 2009   \n",
       "4997  1011918       4  1881149672  Fri May 22 03:45:55 PDT 2009   \n",
       "4998  1566265       4  2187731611  Mon Jun 15 20:43:13 PDT 2009   \n",
       "4999  1038542       4  1956687871  Thu May 28 22:26:41 PDT 2009   \n",
       "\n",
       "                user                                               text  \\\n",
       "0        WookieeChew     @jerryfee Interesting....I'll give it a shot!    \n",
       "1       xokristinaxo  Just got back from the Puerto Rican Fest. Mmmm...   \n",
       "2      Curlybird1988  working today but 2 days off from tomro... lif...   \n",
       "3            JiMpiSh  @tommytrc @tvorse @LisaRedShoesPR The more I h...   \n",
       "4         samsonlion  What a beautiful day. Too bad I missed half of...   \n",
       "...              ...                                                ...   \n",
       "4995  xoxolinzieoxox  i cnt believe how much tweets ive done bout th...   \n",
       "4996   Cranialstrain  @o2 Guys, loving my iPhone but have to say the...   \n",
       "4997        reetsjel  &quot;Do You Feel The Way You Hate? Do You Hat...   \n",
       "4998     ZolaStapler  My blood test was excellent    I had my last c...   \n",
       "4999         pcarles  canard65 has just commented in Fr my blog post...   \n",
       "\n",
       "      weekday_Mon  weekday_Tue  weekday_Wed  weekday_Thu  ...  tfidf_ð½ð¾ðµ  \\\n",
       "0             0.0          1.0          0.0          0.0  ...           0.0   \n",
       "1             0.0          0.0          0.0          0.0  ...           0.0   \n",
       "2             0.0          1.0          0.0          0.0  ...           0.0   \n",
       "3             0.0          0.0          0.0          0.0  ...           0.0   \n",
       "4             0.0          0.0          0.0          0.0  ...           0.0   \n",
       "...           ...          ...          ...          ...  ...           ...   \n",
       "4995          0.0          0.0          0.0          0.0  ...           0.0   \n",
       "4996          0.0          0.0          0.0          0.0  ...           0.0   \n",
       "4997          0.0          0.0          0.0          0.0  ...           0.0   \n",
       "4998          1.0          0.0          0.0          0.0  ...           0.0   \n",
       "4999          0.0          0.0          0.0          1.0  ...           0.0   \n",
       "\n",
       "      tfidf_ð½ñ  tfidf_ð¾ tfidf_ð¾ð tfidf_ð¾ð²  tfidf_ð¾ðµ  tfidf_ð¾ðµð½  \\\n",
       "0           0.0       0.0       0.0        0.0         0.0           0.0   \n",
       "1           0.0       0.0       0.0        0.0         0.0           0.0   \n",
       "2           0.0       0.0       0.0        0.0         0.0           0.0   \n",
       "3           0.0       0.0       0.0        0.0         0.0           0.0   \n",
       "4           0.0       0.0       0.0        0.0         0.0           0.0   \n",
       "...         ...       ...       ...        ...         ...           ...   \n",
       "4995        0.0       0.0       0.0        0.0         0.0           0.0   \n",
       "4996        0.0       0.0       0.0        0.0         0.0           0.0   \n",
       "4997        0.0       0.0       0.0        0.0         0.0           0.0   \n",
       "4998        0.0       0.0       0.0        0.0         0.0           0.0   \n",
       "4999        0.0       0.0       0.0        0.0         0.0           0.0   \n",
       "\n",
       "      tfidf_ð¾ð½ð¾ð¹  tfidf_ð¾ð½ñ  tfidf_ð¾ñ  \n",
       "0                0.0          0.0        0.0  \n",
       "1                0.0          0.0        0.0  \n",
       "2                0.0          0.0        0.0  \n",
       "3                0.0          0.0        0.0  \n",
       "4                0.0          0.0        0.0  \n",
       "...              ...          ...        ...  \n",
       "4995             0.0          0.0        0.0  \n",
       "4996             0.0          0.0        0.0  \n",
       "4997             0.0          0.0        0.0  \n",
       "4998             0.0          0.0        0.0  \n",
       "4999             0.0          0.0        0.0  \n",
       "\n",
       "[5000 rows x 15926 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_sub_dataset, get_sub_featured_datasets\n",
    "DATA_PATH = \"/\".join([my_globals.DATA_DIR, my_globals.MAIN_DATA_NAME])\n",
    "data = get_sub_featured_datasets(size = 5000, random_seed=4)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/\".join([my_globals.DATA_DIR, \"features.csv\"])\n",
    "\n",
    "data.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing \n",
    "steps include:\n",
    "- Link deletion\n",
    "- Username mentions deletion\n",
    "- Decontraction (\"can't\" -> \"cannot\")\n",
    "- Lemmetization (\"shoes\" -> \"shoe\"; \"mice\" -> \"mouse\")\n",
    "- Stopwords deletion (*e.g.* \"a\", \"the\", \"to\", \"of\", etc.)\n",
    "- Punctuation deletion\n",
    "- Digits deletion\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`preprocessing.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/preprocessing.py) content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_globals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "from functools import reduce\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "def setup_nltk():\n",
    "    \"\"\"Downloads necessary packages within nltk\"\"\"\n",
    "    packages = [\"punkt\", \"wordnet\", \"stopwords\"]\n",
    "    for p in packages:\n",
    "        try:\n",
    "            nltk.data.find(p)\n",
    "        except LookupError:\n",
    "            nltk.download(p)\n",
    "\n",
    "\n",
    "def tokenize(s: str, how=\"word_tokenize\") -> List[str]:\n",
    "    if how == \"word_tokenize\":\n",
    "        return word_tokenize(s)\n",
    "    elif how == \"split\":\n",
    "        return s.split()\n",
    "\n",
    "\n",
    "def del_username(s: str) -> str:\n",
    "    \"\"\"Delete @Username from a tweet str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join([t for t in tokenize(s, how=\"split\") if not t.startswith(\"@\")])\n",
    "\n",
    "\n",
    "def del_punc(s: str) -> str:\n",
    "    \"\"\"Delete punctuations from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    punc = my_globals.PUNCS\n",
    "    return \"\".join([w for w in s if w not in punc])\n",
    "\n",
    "\n",
    "def del_link(s: str) -> str:\n",
    "    \"\"\"Delete links from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    r = r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)\"\n",
    "    return \" \".join([re.sub(r, \"\", t) for t in tokenize(s, how=\"split\")])\n",
    "\n",
    "\n",
    "def decontract(s: str) -> str:\n",
    "    \"\"\"Remove contractions in text.\n",
    "    e.g. I'm -> I am; she'd -> she would\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for t in tokenize(s, how=\"split\"):\n",
    "        tokens.append(contractions.fix(t))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def del_stopwords(s: str) -> str:\n",
    "    \"\"\"Delete stopwords and punctuation from a string.\n",
    "    Note that the type-hinting indicates that this function ought\n",
    "    to be run first in the pre-processing pipeline.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    return \" \".join([t for t in tokenize(s) if t not in stop_words])\n",
    "\n",
    "\n",
    "def del_digits(s: str) -> str:\n",
    "    \"\"\"Delete digits from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    return \" \".join([w for w in tokenize(s) if not w.isdigit()])\n",
    "\n",
    "\n",
    "def lemmatize(s: str) -> str:\n",
    "    \"\"\"Lemmatize str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(t) for t in tokenize(s)])\n",
    "\n",
    "\n",
    "def preprocess_pipeline(s: str, return_lower = True) -> str:\n",
    "    \"\"\"Run string through all pre-processing functions.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    s = reduce(\n",
    "        lambda value, function: function(value),\n",
    "        (\n",
    "            del_link,\n",
    "            del_username,\n",
    "            decontract,\n",
    "            lemmatize,\n",
    "            del_stopwords,\n",
    "            del_punc,\n",
    "            del_digits\n",
    "        ),\n",
    "        s,\n",
    "    )\n",
    "\n",
    "    return s.lower() if return_lower else s\n",
    "\n",
    "def str_datetime(s: str):\n",
    "    \"\"\"Parse and format a datetime str to weekday and datetime.MAXYEAR\n",
    "    \n",
    "    :param s: input string containing datetime information\n",
    "    :type s: str\n",
    "    :rtype: tuple[str, str]\n",
    "    \"\"\"\n",
    "    ss = parse(s).strftime('%a %Y-%m-%d %H:%M:%S')\n",
    "    return ss[:3], ss[4:]\n",
    "\n",
    "\n",
    "def cleaning(df: pd.DataFrame):\n",
    "    \"\"\"Cleaning script of the data (or subset).\n",
    "    \n",
    "    :param df: input dataframe.\n",
    "    :type df: pd.DataFrame\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Parse weekday and datetime\n",
    "    weekday_datetime = pd.DataFrame(\n",
    "        list(df.loc[:, \"date\"].apply(str_datetime)),\n",
    "        columns=[\"weekday\", \"datetime\"]\n",
    "    )\n",
    "    # One-hot encode weekday\n",
    "    weekdaydummies = pd.get_dummies(\n",
    "        weekday_datetime['weekday'], \n",
    "        prefix='weekday', \n",
    "        dtype=float\n",
    "    )\n",
    "    weekdaydummies = pd.DataFrame(\n",
    "        weekdaydummies, \n",
    "        columns=['weekday_'+w for w in [\n",
    "            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"\n",
    "        ]]\n",
    "    )\n",
    "    # Concatenate weekday dummies to other features\n",
    "    weekdaydummies_datetime = pd.concat(\n",
    "        [weekdaydummies, weekday_datetime['datetime']], \n",
    "        axis=1\n",
    "    )\n",
    "    df = pd.concat([df, weekdaydummies_datetime], axis=1)\n",
    "    # Drop the column with single unique value.\n",
    "    df.drop(\"flag\", axis = 1, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "proposed features include:\n",
    "- \"!\" usage frequency\n",
    "- \"@\" usage count \n",
    "- capitalized letter usage count\n",
    "- TF-IDF (Term Frequency - Inverse Document Frequency) from processed text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`feature_engineering.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/feature_engineering.py) content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing import preprocess_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "def exlaim_freq(s: str) -> float:\n",
    "    \"\"\"Frequency of excalamtion points in a tweet.\n",
    "\n",
    "    :param s: input str\n",
    "    :type s: str\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    s = \"\".join(s.split())\n",
    "    count = sum([1 if t == \"!\" else 0 for t in s])\n",
    "    return count / len(s)\n",
    "\n",
    "\n",
    "def mention_count(s: str) -> int:\n",
    "    \"\"\"Counts how many mentions occurred in a tweet.\n",
    "\n",
    "    :param s: input str\n",
    "    :type s: str\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    count = sum([1 if t.startswith(\"@\") else 0 for t in s.split()])\n",
    "    return count\n",
    "\n",
    "\n",
    "def cap_freq(s: str) -> float:\n",
    "    \"\"\"Frequency of capitalized letter usage in a tweet.\n",
    "\n",
    "    :param s: input str\n",
    "    :type s: str\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    s = preprocess_pipeline(s)\n",
    "    count = sum([1 if t.isupper() else 0 for t in s])\n",
    "    return count / len(s)\n",
    "\n",
    "\n",
    "def get_tfidf(data: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Encode a Series of text string to TF-IDF.\n",
    "\n",
    "    :param data: input data\n",
    "    :type data: pd.Series\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    tfidf = TfidfTransformer()\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    X = tfidf.fit_transform(X)\n",
    "    return X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
