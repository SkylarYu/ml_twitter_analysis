{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/rl3279/twitter_sentiment_analysis for organized code. \n",
    "\n",
    "Main code used in this notebook is distributed into two files:\n",
    "- [`preprocessing.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/preprocessing.py), containing all scripts to clean and to process raw tweeter text string. \n",
    "- [`feature_engineering.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/feature_engineering.py), containing scripts to extract features from the processed text.\n",
    "\n",
    "For reference, the exact content of these files will be attached at the very end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_globals\n",
    "from preprocessing import cleaning, preprocess_pipeline\n",
    "import feature_engineering as fe\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will only showcase the cleaning, preprocessing, feature-engineering workflow on a subset of data, since the resulting dataset is enormous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linru\\AppData\\Roaming\\Python\\Python39\\site-packages\\dateutil\\parser\\_parser.py:1213: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>weekday_Mon</th>\n",
       "      <th>weekday_Tue</th>\n",
       "      <th>weekday_Wed</th>\n",
       "      <th>weekday_Thu</th>\n",
       "      <th>weekday_Fri</th>\n",
       "      <th>...</th>\n",
       "      <th>tok_ðºð</th>\n",
       "      <th>tok_ð½ð</th>\n",
       "      <th>tok_ð½ñ</th>\n",
       "      <th>tok_ð¾ð</th>\n",
       "      <th>tok_ð¾ð²ð</th>\n",
       "      <th>tok_ð¾ñ</th>\n",
       "      <th>tok_øªø</th>\n",
       "      <th>tok_øªù</th>\n",
       "      <th>tok_ø³</th>\n",
       "      <th>tok_ø¹øª</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2043910798</td>\n",
       "      <td>Fri Jun 05 08:35:39 PDT 2009</td>\n",
       "      <td>ForgottenBeauty</td>\n",
       "      <td>New blog post on 1000Markets - http://bit.ly/1...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1982377745</td>\n",
       "      <td>Sun May 31 11:05:17 PDT 2009</td>\n",
       "      <td>jtothe9</td>\n",
       "      <td>Pink  Need to buy suncream.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2067605566</td>\n",
       "      <td>Sun Jun 07 12:38:27 PDT 2009</td>\n",
       "      <td>Paige_Leigh</td>\n",
       "      <td>Off to the park with Beebs for some frisbee an...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2070574930</td>\n",
       "      <td>Sun Jun 07 17:56:23 PDT 2009</td>\n",
       "      <td>Ales_Alessandra</td>\n",
       "      <td>@imbarbarella It seems that Josie had some hat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2062409302</td>\n",
       "      <td>Sat Jun 06 23:09:04 PDT 2009</td>\n",
       "      <td>baldeggie</td>\n",
       "      <td>@MakikiGirl : girl I was thinking about it but...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4</td>\n",
       "      <td>1969266302</td>\n",
       "      <td>Fri May 29 23:30:20 PDT 2009</td>\n",
       "      <td>chelsearae28</td>\n",
       "      <td>whoa 10 days since an updated status. who care...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4</td>\n",
       "      <td>1687125577</td>\n",
       "      <td>Sun May 03 07:50:27 PDT 2009</td>\n",
       "      <td>catiams</td>\n",
       "      <td>@sharlynnx dont worry, we will get their atten...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4</td>\n",
       "      <td>2057655317</td>\n",
       "      <td>Sat Jun 06 13:24:42 PDT 2009</td>\n",
       "      <td>ids</td>\n",
       "      <td>@claycock I hereby sentence you to being trapp...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>1751431150</td>\n",
       "      <td>Sat May 09 19:26:03 PDT 2009</td>\n",
       "      <td>Cunby30</td>\n",
       "      <td>Wondering what i'm doing to do about my iphone...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4</td>\n",
       "      <td>2055743236</td>\n",
       "      <td>Sat Jun 06 09:52:32 PDT 2009</td>\n",
       "      <td>__aisling</td>\n",
       "      <td>@lauradawg i love the ageless style issues!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 8148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target         ids                          date             user  \\\n",
       "0          4  2043910798  Fri Jun 05 08:35:39 PDT 2009  ForgottenBeauty   \n",
       "1          0  1982377745  Sun May 31 11:05:17 PDT 2009          jtothe9   \n",
       "2          4  2067605566  Sun Jun 07 12:38:27 PDT 2009      Paige_Leigh   \n",
       "3          0  2070574930  Sun Jun 07 17:56:23 PDT 2009  Ales_Alessandra   \n",
       "4          0  2062409302  Sat Jun 06 23:09:04 PDT 2009        baldeggie   \n",
       "...      ...         ...                           ...              ...   \n",
       "4995       4  1969266302  Fri May 29 23:30:20 PDT 2009     chelsearae28   \n",
       "4996       4  1687125577  Sun May 03 07:50:27 PDT 2009          catiams   \n",
       "4997       4  2057655317  Sat Jun 06 13:24:42 PDT 2009              ids   \n",
       "4998       0  1751431150  Sat May 09 19:26:03 PDT 2009          Cunby30   \n",
       "4999       4  2055743236  Sat Jun 06 09:52:32 PDT 2009        __aisling   \n",
       "\n",
       "                                                   text  weekday_Mon  \\\n",
       "0     New blog post on 1000Markets - http://bit.ly/1...          0.0   \n",
       "1                           Pink  Need to buy suncream.          0.0   \n",
       "2     Off to the park with Beebs for some frisbee an...          0.0   \n",
       "3     @imbarbarella It seems that Josie had some hat...          0.0   \n",
       "4     @MakikiGirl : girl I was thinking about it but...          0.0   \n",
       "...                                                 ...          ...   \n",
       "4995  whoa 10 days since an updated status. who care...          0.0   \n",
       "4996  @sharlynnx dont worry, we will get their atten...          0.0   \n",
       "4997  @claycock I hereby sentence you to being trapp...          0.0   \n",
       "4998  Wondering what i'm doing to do about my iphone...          0.0   \n",
       "4999       @lauradawg i love the ageless style issues!           0.0   \n",
       "\n",
       "      weekday_Tue  weekday_Wed  weekday_Thu  weekday_Fri  ...  tok_ðºð  \\\n",
       "0             0.0          0.0          0.0          1.0  ...      0.0   \n",
       "1             0.0          0.0          0.0          0.0  ...      0.0   \n",
       "2             0.0          0.0          0.0          0.0  ...      0.0   \n",
       "3             0.0          0.0          0.0          0.0  ...      0.0   \n",
       "4             0.0          0.0          0.0          0.0  ...      0.0   \n",
       "...           ...          ...          ...          ...  ...      ...   \n",
       "4995          0.0          0.0          0.0          1.0  ...      0.0   \n",
       "4996          0.0          0.0          0.0          0.0  ...      0.0   \n",
       "4997          0.0          0.0          0.0          0.0  ...      0.0   \n",
       "4998          0.0          0.0          0.0          0.0  ...      0.0   \n",
       "4999          0.0          0.0          0.0          0.0  ...      0.0   \n",
       "\n",
       "      tok_ð½ð tok_ð½ñ tok_ð¾ð  tok_ð¾ð²ð  tok_ð¾ñ  tok_øªø  tok_øªù  tok_ø³  \\\n",
       "0         0.0     0.0     0.0        0.0      0.0      0.0      0.0     0.0   \n",
       "1         0.0     0.0     0.0        0.0      0.0      0.0      0.0     0.0   \n",
       "2         0.0     0.0     0.0        0.0      0.0      0.0      0.0     0.0   \n",
       "3         0.0     0.0     0.0        0.0      0.0      0.0      0.0     0.0   \n",
       "4         0.0     0.0     0.0        0.0      0.0      0.0      0.0     0.0   \n",
       "...       ...     ...     ...        ...      ...      ...      ...     ...   \n",
       "4995      0.0     0.0     0.0        0.0      0.0      0.0      0.0     0.0   \n",
       "4996      0.0     0.0     0.0        0.0      0.0      0.0      0.0     0.0   \n",
       "4997      0.0     0.0     0.0        0.0      0.0      0.0      0.0     0.0   \n",
       "4998      0.0     0.0     0.0        0.0      0.0      0.0      0.0     0.0   \n",
       "4999      0.0     0.0     0.0        0.0      0.0      0.0      0.0     0.0   \n",
       "\n",
       "      tok_ø¹øª  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "...        ...  \n",
       "4995       0.0  \n",
       "4996       0.0  \n",
       "4997       0.0  \n",
       "4998       0.0  \n",
       "4999       0.0  \n",
       "\n",
       "[5000 rows x 8148 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_sub_dataset\n",
    "DATA_PATH = \"/\".join([my_globals.DATA_DIR, my_globals.MAIN_DATA_NAME])\n",
    "\n",
    "get_sub_dataset(size = 5000, random_seed=42)\n",
    "data = pd.read_csv(\n",
    "    \"/\".join([my_globals.DATA_DIR, \"twitter_seed42.csv\"]),\n",
    "    encoding = \"latin1\",\n",
    "    header = 0, \n",
    ")\n",
    "\n",
    "\n",
    "data = cleaning(data)\n",
    "data[\"processed_text\"] = data[\"text\"].apply(preprocess_pipeline)\n",
    "\n",
    "data[\"exclaim_freq\"] = data[\"text\"].apply(fe.exclaim_freq)\n",
    "data[\"mention_count\"] = data[\"text\"].apply(fe.mention_count)\n",
    "data[\"cap_freq\"] = data[\"text\"].apply(fe.cap_freq)\n",
    "tfidf = fe.get_tfidf(data[\"processed_text\"])\n",
    "data = pd.concat([data, tfidf], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/\".join([my_globals.DATA_DIR, \"features.csv\"])\n",
    "\n",
    "data.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing \n",
    "steps include:\n",
    "- Link deletion\n",
    "- Username mentions deletion\n",
    "- Decontraction (\"can't\" -> \"cannot\")\n",
    "- Lemmetization (\"shoes\" -> \"shoe\"; \"mice\" -> \"mouse\")\n",
    "- Stopwords deletion (*e.g.* \"a\", \"the\", \"to\", \"of\", etc.)\n",
    "- Punctuation deletion\n",
    "- Digits deletion\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`preprocessing.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/preprocessing.py) content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_globals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "from functools import reduce\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "def setup_nltk():\n",
    "    \"\"\"Downloads necessary packages within nltk\"\"\"\n",
    "    packages = [\"punkt\", \"wordnet\", \"stopwords\"]\n",
    "    for p in packages:\n",
    "        try:\n",
    "            nltk.data.find(p)\n",
    "        except LookupError:\n",
    "            nltk.download(p)\n",
    "\n",
    "\n",
    "def tokenize(s: str, how=\"word_tokenize\") -> List[str]:\n",
    "    if how == \"word_tokenize\":\n",
    "        return word_tokenize(s)\n",
    "    elif how == \"split\":\n",
    "        return s.split()\n",
    "\n",
    "\n",
    "def del_username(s: str) -> str:\n",
    "    \"\"\"Delete @Username from a tweet str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join([t for t in tokenize(s, how=\"split\") if not t.startswith(\"@\")])\n",
    "\n",
    "\n",
    "def del_punc(s: str) -> str:\n",
    "    \"\"\"Delete punctuations from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    punc = my_globals.PUNCS\n",
    "    return \"\".join([w for w in s if w not in punc])\n",
    "\n",
    "\n",
    "def del_link(s: str) -> str:\n",
    "    \"\"\"Delete links from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    r = r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)\"\n",
    "    return \" \".join([re.sub(r, \"\", t) for t in tokenize(s, how=\"split\")])\n",
    "\n",
    "\n",
    "def decontract(s: str) -> str:\n",
    "    \"\"\"Remove contractions in text.\n",
    "    e.g. I'm -> I am; she'd -> she would\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for t in tokenize(s, how=\"split\"):\n",
    "        tokens.append(contractions.fix(t))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def del_stopwords(s: str) -> str:\n",
    "    \"\"\"Delete stopwords and punctuation from a string.\n",
    "    Note that the type-hinting indicates that this function ought\n",
    "    to be run first in the pre-processing pipeline.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    return \" \".join([t for t in tokenize(s) if t not in stop_words])\n",
    "\n",
    "\n",
    "def del_digits(s: str) -> str:\n",
    "    \"\"\"Delete digits from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    return \" \".join([w for w in tokenize(s) if not w.isdigit()])\n",
    "\n",
    "\n",
    "def lemmatize(s: str) -> str:\n",
    "    \"\"\"Lemmatize str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(t) for t in tokenize(s)])\n",
    "\n",
    "\n",
    "def preprocess_pipeline(s: str, return_lower = True) -> str:\n",
    "    \"\"\"Run string through all pre-processing functions.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    s = reduce(\n",
    "        lambda value, function: function(value),\n",
    "        (\n",
    "            del_link,\n",
    "            del_username,\n",
    "            decontract,\n",
    "            lemmatize,\n",
    "            del_stopwords,\n",
    "            del_punc,\n",
    "            del_digits\n",
    "        ),\n",
    "        s,\n",
    "    )\n",
    "\n",
    "    return s.lower() if return_lower else s\n",
    "\n",
    "def str_datetime(s: str):\n",
    "    \"\"\"Parse and format a datetime str to weekday and datetime.MAXYEAR\n",
    "    \n",
    "    :param s: input string containing datetime information\n",
    "    :type s: str\n",
    "    :rtype: tuple[str, str]\n",
    "    \"\"\"\n",
    "    ss = parse(s).strftime('%a %Y-%m-%d %H:%M:%S')\n",
    "    return ss[:3], ss[4:]\n",
    "\n",
    "\n",
    "def cleaning(df: pd.DataFrame):\n",
    "    \"\"\"Cleaning script of the data (or subset).\n",
    "    \n",
    "    :param df: input dataframe.\n",
    "    :type df: pd.DataFrame\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Parse weekday and datetime\n",
    "    weekday_datetime = pd.DataFrame(\n",
    "        list(df.loc[:, \"date\"].apply(str_datetime)),\n",
    "        columns=[\"weekday\", \"datetime\"]\n",
    "    )\n",
    "    # One-hot encode weekday\n",
    "    weekdaydummies = pd.get_dummies(\n",
    "        weekday_datetime['weekday'], \n",
    "        prefix='weekday', \n",
    "        dtype=float\n",
    "    )\n",
    "    weekdaydummies = pd.DataFrame(\n",
    "        weekdaydummies, \n",
    "        columns=['weekday_'+w for w in [\n",
    "            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"\n",
    "        ]]\n",
    "    )\n",
    "    # Concatenate weekday dummies to other features\n",
    "    weekdaydummies_datetime = pd.concat(\n",
    "        [weekdaydummies, weekday_datetime['datetime']], \n",
    "        axis=1\n",
    "    )\n",
    "    df = pd.concat([df, weekdaydummies_datetime], axis=1)\n",
    "    # Drop the column with single unique value.\n",
    "    df.drop(\"flag\", axis = 1, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "proposed features include:\n",
    "- \"!\" usage frequency\n",
    "- \"@\" usage count \n",
    "- capitalized letter usage count\n",
    "- TF-IDF (Term Frequency - Inverse Document Frequency) from processed text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`feature_engineering.py`](https://github.com/rl3279/twitter_sentiment_analysis/blob/main/feature_engineering.py) content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing import preprocess_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "def exlaim_freq(s: str) -> float:\n",
    "    \"\"\"Frequency of excalamtion points in a tweet.\n",
    "\n",
    "    :param s: input str\n",
    "    :type s: str\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    s = \"\".join(s.split())\n",
    "    count = sum([1 if t == \"!\" else 0 for t in s])\n",
    "    return count / len(s)\n",
    "\n",
    "\n",
    "def mention_count(s: str) -> int:\n",
    "    \"\"\"Counts how many mentions occurred in a tweet.\n",
    "\n",
    "    :param s: input str\n",
    "    :type s: str\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    count = sum([1 if t.startswith(\"@\") else 0 for t in s.split()])\n",
    "    return count\n",
    "\n",
    "\n",
    "def cap_freq(s: str) -> float:\n",
    "    \"\"\"Frequency of capitalized letter usage in a tweet.\n",
    "\n",
    "    :param s: input str\n",
    "    :type s: str\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    s = preprocess_pipeline(s)\n",
    "    count = sum([1 if t.isupper() else 0 for t in s])\n",
    "    return count / len(s)\n",
    "\n",
    "\n",
    "def get_tfidf(data: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Encode a Series of text string to TF-IDF.\n",
    "\n",
    "    :param data: input data\n",
    "    :type data: pd.Series\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    tfidf = TfidfTransformer()\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    X = tfidf.fit_transform(X)\n",
    "    return X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
